[
  {
    "id": 1,
    "arxiv_id": "2601.10696v1",
    "processed_date": "2026-01-18T11:06:25.127590",
    "html_url": "https://arxiv.org/html/2601.10696v1",
    "published": "2026-01-15T18:52:59+00:00",
    "title": "The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load",
    "authors": [
      "Han Jiang",
      "Yao Xiao",
      "Rachel Hurley",
      "Shichao Liu"
    ],
    "summary": "Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10696v1"
  },
  {
    "id": 2,
    "arxiv_id": "2601.10691v1",
    "processed_date": "2026-01-18T11:06:25.127632",
    "html_url": "https://arxiv.org/html/2601.10691v1",
    "published": "2026-01-15T18:50:48+00:00",
    "title": "The Conversational Exam: A Scalable Assessment Design for the AI Era",
    "authors": [
      "Lorena A. Barba",
      "Laura Stegner"
    ],
    "summary": "Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (students work with documentation and supervised AI access) with inherent validity (real-time performance cannot be faked). We provide detailed implementation guidance to help instructors adapt this approach, offering a practical path forward when many educators feel paralyzed between banning AI entirely or accepting that valid assessment is impossible.",
    "primary_category": "cs.CY",
    "categories": [
      "cs.CY",
      "cs.CE",
      "cs.HC"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10691v1"
  },
  {
    "id": 3,
    "arxiv_id": "2601.10684v1",
    "processed_date": "2026-01-18T11:06:25.127659",
    "html_url": "https://arxiv.org/html/2601.10684v1",
    "published": "2026-01-15T18:46:09+00:00",
    "title": "On the origin of neural scaling laws: from random graphs to natural language",
    "authors": [
      "Maissam Barkeshli",
      "Alberto Alfarano",
      "Andrey Gromov"
    ],
    "summary": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "stat.ML"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10684v1"
  },
  {
    "id": 4,
    "arxiv_id": "2601.10599v1",
    "processed_date": "2026-01-18T11:06:25.127687",
    "html_url": "https://arxiv.org/html/2601.10599v1",
    "published": "2026-01-15T17:08:26+00:00",
    "title": "Institutional AI: A Governance Framework for Distributional AGI Safety",
    "authors": [
      "Federico Pierucci",
      "Marcello Galisai",
      "Marcantonio Syrnikov Bracale",
      "Matteo Prandi",
      "Piercosma Bisconti",
      "Francesco Giarrusso",
      "Olga Sorokoletova",
      "Vincenzo Suriani",
      "Daniele Nardi"
    ],
    "summary": "As LLM-based systems increasingly operate as agents embedded within human social and technical systems, alignment can no longer be treated as a property of an isolated model, but must be understood in relation to the environments in which these agents act. Even the most sophisticated methods of alignment, such as Reinforcement Learning through Human Feedback (RHLF) or through AI Feedback (RLAIF) cannot ensure control once internal goal structures diverge from developer intent. We identify three structural problems that emerge from core properties of AI models: (1) behavioral goal-independence, where models develop internal objectives and misgeneralize goals; (2) instrumental override of natural-language constraints, where models regard safety principles as non-binding while pursuing latent objectives, leveraging deception and manipulation; and (3) agentic alignment drift, where individually aligned agents converge to collusive equilibria through interaction dynamics invisible to single-agent audits. The solution this paper advances is Institutional AI: a system-level approach that treats alignment as a question of effective governance of AI agent collectives. We argue for a governance-graph that details how to constrain agents via runtime monitoring, incentive shaping through prizes and sanctions, explicit norms and enforcement roles. This institutional turn reframes safety from software engineering to a mechanism design problem, where the primary goal of alignment is shifting the payoff landscape of AI agent collectives.",
    "primary_category": "cs.CY",
    "categories": [
      "cs.CY"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10599v1"
  },
  {
    "id": 5,
    "arxiv_id": "2601.10591v1",
    "processed_date": "2026-01-18T11:06:25.127712",
    "html_url": "https://arxiv.org/html/2601.10591v1",
    "published": "2026-01-15T17:02:06+00:00",
    "title": "ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition",
    "authors": [
      "Arundeep Chinta",
      "Lucas Vinh Tran",
      "Jay Katukuri"
    ],
    "summary": "Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.RM",
      "q-fin.TR"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10591v1"
  },
  {
    "id": 6,
    "arxiv_id": "2601.10582v1",
    "processed_date": "2026-01-18T11:06:25.127737",
    "html_url": "https://arxiv.org/html/2601.10582v1",
    "published": "2026-01-15T16:54:34+00:00",
    "title": "Mitigating GIL Bottlenecks in Edge AI Systems",
    "authors": [
      "Mridankan Mandal",
      "Smit Sanjay Shende"
    ],
    "summary": "Deploying Python based AI agents on resource-constrained edge devices presents a runtime optimization challenge: high thread counts are needed to mask I/O latency, yet Python's Global Interpreter Lock (GIL) serializes execution. We demonstrate that naive thread-pool scaling causes a \"saturation cliff\": >= 20% throughput degradation at overprovisioned thread counts (N >= 512) on edge-representative configurations. We present a lightweight profiling tool and adaptive runtime system using a Blocking Ratio metric (beta) that distinguishes genuine I/O wait from GIL contention. Our library-based solution achieves 96.5% of optimal performance without manual tuning, outperforming multiprocessing (limited by ~8x memory overhead on devices with 512 MB-2 GB RAM) and asyncio (blocked by CPU-bound phases). Evaluation across seven edge AI workload profiles, including real ML inference with ONNX Runtime MobileNetV2, demonstrates 93.9% average efficiency. Comparative experiments with Python 3.13t (free threading) show that while GIL elimination enables ~4x throughput on multi-core edge devices, the saturation cliff persists on single-core devices, validating our beta metric for both GIL and no-GIL environments. This provides practical optimization for edge AI systems.",
    "primary_category": "cs.DC",
    "categories": [
      "cs.DC",
      "cs.OS",
      "cs.PF"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10582v1"
  },
  {
    "id": 7,
    "arxiv_id": "2601.10567v1",
    "processed_date": "2026-01-18T11:06:25.127768",
    "html_url": "https://arxiv.org/html/2601.10567v1",
    "published": "2026-01-15T16:29:23+00:00",
    "title": "Generative AI collective behavior needs an interactionist paradigm",
    "authors": [
      "Laura Ferrarotti",
      "Gian Maria Campedelli",
      "Roberto Dessì",
      "Andrea Baronchelli",
      "Giovanni Iacca",
      "Kathleen M. Carley",
      "Alex Pentland",
      "Joel Z. Leibo",
      "James Evans",
      "Bruno Lepri"
    ],
    "summary": "In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10567v1"
  },
  {
    "id": 8,
    "arxiv_id": "2601.10562v1",
    "processed_date": "2026-01-18T11:06:25.127818",
    "html_url": "https://arxiv.org/html/2601.10562v1",
    "published": "2026-01-15T16:25:55+00:00",
    "title": "Process-Guided Concept Bottleneck Model",
    "authors": [
      "Reza M. Asiyabi",
      "SEOSAW Partnership",
      "Steven Hancock",
      "Casey Ryan"
    ],
    "summary": "Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10562v1"
  },
  {
    "id": 9,
    "arxiv_id": "2601.10524v1",
    "processed_date": "2026-01-18T11:06:25.127844",
    "html_url": "https://arxiv.org/html/2601.10524v1",
    "published": "2026-01-15T15:51:24+00:00",
    "title": "Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection",
    "authors": [
      "Frank Bobe",
      "Gregory D. Vetaw",
      "Chase Pavlick",
      "Darshan Bryner",
      "Matthew Cook",
      "Jose Salas-Vernis"
    ],
    "summary": "The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10524v1"
  },
  {
    "id": 10,
    "arxiv_id": "2601.10520v1",
    "processed_date": "2026-01-18T11:06:25.127875",
    "html_url": "https://arxiv.org/html/2601.10520v1",
    "published": "2026-01-15T15:47:38+00:00",
    "title": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment",
    "authors": [
      "Felix Jahn",
      "Yannic Muskalla",
      "Lisa Dargasz",
      "Patrick Schramowski",
      "Kevin Baum"
    ],
    "summary": "As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10520v1"
  }
]