{
  "status": "success",
  "papers_count": 15,
  "papers": [
    {
      "id": 1,
      "arxiv_id": "2602.06043v1",
      "fetched_date": "2026-02-07T16:43:31.417594",
      "published": "2026-02-05T18:59:58+00:00",
      "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
      "authors": [
        "Prakhar Kaushik",
        "Ankit Vaidya",
        "Shravan Chaudhari",
        "Rama Chellappa",
        "Alan Yuille"
      ],
      "summary": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06043v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06043v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06043v1"
    },
    {
      "id": 2,
      "arxiv_id": "2602.06039v1",
      "fetched_date": "2026-02-07T16:43:31.417773",
      "published": "2026-02-05T18:59:51+00:00",
      "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
      "authors": [
        "Yuxing Lu",
        "Yucheng Hu",
        "Xukai Zhao",
        "Jiuxin Cao"
      ],
      "summary": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the managers round goal, each agent outputs lightweight natural-language query (need) and key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. 6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06039v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06039v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06039v1"
    },
    {
      "id": 3,
      "arxiv_id": "2602.06038v1",
      "fetched_date": "2026-02-07T16:43:31.417839",
      "published": "2026-02-05T18:59:45+00:00",
      "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction",
      "authors": [
        "Xiaopan Zhang",
        "Zejin Wang",
        "Zhixu Li",
        "Jianpeng Yao",
        "Jiachen Li"
      ],
      "summary": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https:comm-cp.github.io.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06038v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06038v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06038v1"
    },
    {
      "id": 4,
      "arxiv_id": "2602.06036v1",
      "fetched_date": "2026-02-07T16:43:31.417909",
      "published": "2026-02-05T18:59:30+00:00",
      "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
      "authors": [
        "Jian Chen",
        "Yesheng Liang",
        "Zhijian Liu"
      ],
      "summary": "Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06036v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06036v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06036v1"
    },
    {
      "id": 5,
      "arxiv_id": "2602.06033v1",
      "fetched_date": "2026-02-07T16:43:31.417971",
      "published": "2026-02-05T18:59:20+00:00",
      "title": "Can vision language models learn intuitive physics from interaction?",
      "authors": [
        "Luca M. Schulze Buschoff",
        "Konstantinos Voudouris",
        "Can Demircan",
        "Eric Schulz"
      ],
      "summary": "Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06033v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06033v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06033v1"
    },
    {
      "id": 6,
      "arxiv_id": "2602.06030v1",
      "fetched_date": "2026-02-07T16:43:31.418026",
      "published": "2026-02-05T18:59:01+00:00",
      "title": "PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling",
      "authors": [
        "Kavana Venkatesh",
        "Yinhan He",
        "Jundong Li",
        "Jiaming Cui"
      ],
      "summary": "Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.",
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06030v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06030v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06030v1"
    },
    {
      "id": 7,
      "arxiv_id": "2602.06031v1",
      "fetched_date": "2026-02-07T16:43:31.418093",
      "published": "2026-02-05T18:59:01+00:00",
      "title": "AP-OOD: Attention Pooling for Out-of-Distribution Detection",
      "authors": [
        "Claus Hofmann",
        "Christian Huber",
        "Bernhard Lehner",
        "Daniel Klotz",
        "Sepp Hochreiter",
        "Werner Zellinger"
      ],
      "summary": "Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95 true positives) from 27.84 to 4.67 on XSUM summarization, and from 77.08 to 70.37 on WMT15 En-Fr translation.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06031v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06031v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06031v1"
    },
    {
      "id": 8,
      "arxiv_id": "2602.06029v1",
      "fetched_date": "2026-02-07T16:43:31.418149",
      "published": "2026-02-05T18:58:32+00:00",
      "title": "Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference",
      "authors": [
        "Yingke Li",
        "Anjali Parashar",
        "Enlu Zhou",
        "Chuchu Fan"
      ],
      "summary": "Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06029v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06029v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06029v1"
    },
    {
      "id": 9,
      "arxiv_id": "2602.06025v1",
      "fetched_date": "2026-02-07T16:43:31.418214",
      "published": "2026-02-05T18:57:09+00:00",
      "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
      "authors": [
        "Haozhen Zhang",
        "Haodong Yue",
        "Tao Feng",
        "Quanyu Long",
        "Jianzhu Bao",
        "Bowen Jin",
        "Weizhi Zhang",
        "Xiao Li",
        "Jiaxuan You",
        "Chengwei Qin",
        "Wenya Wang"
      ],
      "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present textbfBudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., textscLowtextscMidtextscHigh). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06025v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06025v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06025v1"
    },
    {
      "id": 10,
      "arxiv_id": "2602.06023v1",
      "fetched_date": "2026-02-07T16:43:31.418284",
      "published": "2026-02-05T18:56:49+00:00",
      "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments",
      "authors": [
        "Christopher A. McClurg",
        "Alan R. Wagner"
      ],
      "summary": "Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06023v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06023v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06023v1"
    },
    {
      "id": 11,
      "arxiv_id": "2602.06022v1",
      "fetched_date": "2026-02-07T16:43:31.418343",
      "published": "2026-02-05T18:55:56+00:00",
      "title": "Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering",
      "authors": [
        "Miranda Muqing Miao",
        "Young-Min Cho",
        "Lyle Ungar"
      ],
      "summary": "Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10 and expected calibration error (ECE) by 50 on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14 accuracy improvements and 49 ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06022v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06022v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06022v1"
    },
    {
      "id": 12,
      "arxiv_id": "2602.06021v1",
      "fetched_date": "2026-02-07T16:43:31.418406",
      "published": "2026-02-05T18:55:03+00:00",
      "title": "Diffusion Model's Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold",
      "authors": [
        "Ye He",
        "Yitong Qiu",
        "Molei Tao"
      ],
      "summary": "When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the models performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion models inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.NA",
        "math.PR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06021v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06021v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06021v1"
    },
    {
      "id": 13,
      "arxiv_id": "2602.06020v1",
      "fetched_date": "2026-02-07T16:43:31.418478",
      "published": "2026-02-05T18:54:54+00:00",
      "title": "Mechanisms of AI Protein Folding in ESMFold",
      "authors": [
        "Kevin Lu",
        "Jannik Brinkmann",
        "Stefan Huber",
        "Aaron Mueller",
        "Yonatan Belinkov",
        "David Bau",
        "Chris Wendler"
      ],
      "summary": "How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "q-bio.BM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06020v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06020v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06020v1"
    },
    {
      "id": 14,
      "arxiv_id": "2602.06019v1",
      "fetched_date": "2026-02-07T16:43:31.418528",
      "published": "2026-02-05T18:54:48+00:00",
      "title": "Multi-Token Prediction via Self-Distillation",
      "authors": [
        "John Kirchenbauer",
        "Abhimanyu Hans",
        "Brian Bartoldson",
        "Micah Goldblum",
        "Ashwinee Panda",
        "Tom Goldstein"
      ],
      "summary": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than 3times faster on average at 5 drop in accuracy relative to single token decoding performance.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06019v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06019v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06019v1"
    },
    {
      "id": 15,
      "arxiv_id": "2602.06015v1",
      "fetched_date": "2026-02-07T16:43:31.418584",
      "published": "2026-02-05T18:53:17+00:00",
      "title": "A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies",
      "authors": [
        "Panagiotis Kaliosis",
        "Adithya V Ganesan",
        "Oscar N. E. Kjell",
        "Whitney Ringwald",
        "Scott Feltman",
        "Melissa A. Carr",
        "Dimitris Samaras",
        "Camilo Ruggero",
        "Benjamin J. Luft",
        "Roman Kotov",
        "Andrew H. Schwartz"
      ],
      "summary": "Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.06015v1",
      "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2602.06015v1",
      "arxiv_url": "https://arxiv.org/abs/2602.06015v1"
    }
  ],
  "query_used": "(cat:cs.AI OR cat:cs.LG OR cat:cs.CL) AND (large language models OR transformers OR attention mechanism)",
  "categories_searched": [
    "cs.AI",
    "cs.LG",
    "cs.CL"
  ]
}