[
  {
    "id": 1,
    "arxiv_id": "2601.05229v1",
    "processed_date": "2026-01-11T16:48:41.726642",
    "published": "2026-01-08T18:55:01+00:00",
    "title": "Mitigating Simulator Dependence in AI Parameter Inference for the Epoch of Reionization: The Importance of Simulation Diversity",
    "authors": [
      "Jasper Solt",
      "Jonathan C. Pober",
      "Stephen H. Bach"
    ],
    "summary": "The 21cm signal of neutral hydrogen contains a wealth of information about the poorly constrained era of cosmological history, the Epoch of Reionization (EoR). Recently, AI models trained on EoR simulations have gained significant attention as a powerful and flexible option for inferring parameters from 21cm observations. However, previous works show that AI models trained on data from one simulator fail to generalize to data from another, raising doubts about AI models' ability to accurately infer parameters from observation. We develop a new strategy for training AI models on cosmological simulations based on the principle that increasing the diversity of the training dataset improves model robustness by averaging out spurious and contradictory information. We train AI models on data from different combinations of four simulators, then compare the models' performance when predicting on data from held-out simulators acting as proxies for the real universe. We find that models trained on data from multiple simulators perform better on data from a held-out simulator than models trained on data from a single simulator, indicating that increasing the diversity of the training dataset improves a model's ability to generalize. This result suggests that future EoR parameter inference methods can mitigate simulator-specific bias by incorporating multiple simulation approaches into their analyses.",
    "primary_category": "astro-ph.CO",
    "categories": [
      "astro-ph.CO"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05229v1"
  },
  {
    "id": 2,
    "arxiv_id": "2601.05205v1",
    "processed_date": "2026-01-11T16:48:41.726682",
    "published": "2026-01-08T18:31:11+00:00",
    "title": "EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI",
    "authors": [
      "Zain Iqbal",
      "Lorenzo Valerio"
    ],
    "summary": "Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.PF"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05205v1"
  },
  {
    "id": 3,
    "arxiv_id": "2601.05191v1",
    "processed_date": "2026-01-11T16:48:41.726712",
    "published": "2026-01-08T18:13:46+00:00",
    "title": "Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable",
    "authors": [
      "Zuhair Ahmed Khan Taha",
      "Mohammed Mudassir Uddin",
      "Shahnawaz Alam"
    ],
    "summary": "When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines",
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05191v1"
  },
  {
    "id": 4,
    "arxiv_id": "2601.05187v1",
    "processed_date": "2026-01-11T16:48:41.726736",
    "published": "2026-01-08T18:10:35+00:00",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "authors": [
      "Yanchang Liang",
      "Xiaowei Zhao"
    ],
    "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05187v1"
  },
  {
    "id": 5,
    "arxiv_id": "2601.05162v1",
    "processed_date": "2026-01-11T16:48:41.726761",
    "published": "2026-01-08T17:51:35+00:00",
    "title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation",
    "authors": [
      "Jinze Yu",
      "Dayuan Jiang"
    ],
    "summary": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.",
    "primary_category": "cs.GR",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05162v1"
  },
  {
    "id": 6,
    "arxiv_id": "2601.05149v1",
    "processed_date": "2026-01-11T16:48:41.726785",
    "published": "2026-01-08T17:39:35+00:00",
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "authors": [
      "Elia Peruzzo",
      "Guillaume Sauti√®re",
      "Amirhossein Habibian"
    ],
    "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\\mathbf{1.7\\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05149v1"
  },
  {
    "id": 7,
    "arxiv_id": "2601.05111v1",
    "processed_date": "2026-01-11T16:48:41.726811",
    "published": "2026-01-08T16:58:10+00:00",
    "title": "Agent-as-a-Judge",
    "authors": [
      "Runyang You",
      "Hongru Cai",
      "Caiqi Zhang",
      "Qiancheng Xu",
      "Meng Liu",
      "Tiezheng Yu",
      "Yongqi Li",
      "Wenjie Li"
    ],
    "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05111v1"
  },
  {
    "id": 8,
    "arxiv_id": "2601.05104v1",
    "processed_date": "2026-01-11T16:48:41.726838",
    "published": "2026-01-08T16:50:00+00:00",
    "title": "How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness",
    "authors": [
      "Florence Bernays",
      "Marco Henriques Pereira",
      "Jochen Menges"
    ],
    "summary": "This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.",
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "econ.GN"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05104v1"
  },
  {
    "id": 9,
    "arxiv_id": "2601.05101v1",
    "processed_date": "2026-01-11T16:48:41.726864",
    "published": "2026-01-08T16:47:09+00:00",
    "title": "Arabic Prompts with English Tools: A Benchmark",
    "authors": [
      "Konstantin Kubrak",
      "Ahmed El-Moselhy",
      "Ammar Alsulami",
      "Remaz Altuwaim",
      "Hassan Ismail Fawaz",
      "Faisal Alsaby"
    ],
    "summary": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05101v1"
  },
  {
    "id": 10,
    "arxiv_id": "2601.05091v1",
    "processed_date": "2026-01-11T16:48:41.726889",
    "published": "2026-01-08T16:39:26+00:00",
    "title": "Code-Mix Sentiment Analysis on Hinglish Tweets",
    "authors": [
      "Aashi Garg",
      "Aneshya Das",
      "Arshi Arya",
      "Anushka Goyal",
      "Aditi"
    ],
    "summary": "The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.",
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "link": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05091v1"
  }
]